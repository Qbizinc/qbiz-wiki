<!--
title: GCP Professional Machine Learning Engineer
description: 
published: true
date: 2023-04-17T16:30:08.535Z
tags: 
editor: ckeditor
dateCreated: 2023-01-18T19:37:50.189Z
-->

<h1>GCP Professional Machine Learning Engineer</h1>
<h2>How to get access to the material?</h2>
<p>With QBiz, we're using GCP partner portal to access all the GCP trainings. To be able to register to <strong>Certification Learning Path: Professional Machine Learning Engineer </strong>you need to follow<strong> </strong>the path:</p>
<ol>
  <li>Ask Mayra Madrigal to send you GCP partner portal link.</li>
  <li>You'll receive an email with the title <strong>Welcome to Google Cloud Partner Advantage! </strong>Follow the “Login to the portal” link in the email, and login with your QBiz credentials.</li>
  <li>Under the tab “Training” you can see a list of Google Cloud certifications portfolio trainings, one of them being <strong>Professional Machine Learning Engineer. </strong>At least for me, the access was denied for all the certification trainings at this point.</li>
  <li>To get access to Machine Learning training, you need to follow the instructions in<a href="https://qbiz-wiki.com/en/training/Certifications/Google/Kickstart"> https://qbiz-wiki.com/en/training/Certifications/Google/Kickstart</a> . You'll notice there is no option for Machine Leaning in the Partner Certification Kickstart tab, so you can register for Professional Data Engineer training. It goes against intuition, but following this process, the Machine Learning trainings became accessible.</li>
</ol>
<h2>Estimated duration of the training is 40 days</h2>
<p>There is no easy way to see the estimated duration of all of the trainings in the GCP portal, you'll need to click each of the courses to see the estimated time of each part. That's why I outline the time estimates here:</p>
<ul>
  <li>Big Data and ML Fundamentals (1 day)</li>
  <li>How Google does Machine Learning 1 day)</li>
  <li>Launching into Machine Learning 4 days)</li>
  <li>TensorFlow on Google Cloud (3 days)</li>
  <li>Feature Engineering (3 days)</li>
  <li>Machine Learning in the Enterprise (4 days)</li>
  <li>Production Machine Learning Systems (2 days)</li>
  <li>Computer Vision Fundamentals with Google Cloud (GCP: No estimate. My rough estimate is 5 days)</li>
  <li>Sequence Models for Time Series and Natural Language Processing (1 day)</li>
  <li>Recommendation Systems with TensorFlow on Google Cloud (GCP estimate is 1 day, but it might not be accurate, as labs alone seem to take 9 h)</li>
  <li>MLOps (Machine Learning Operations) Fundamentals (5 days)</li>
  <li>ML Pipelines on Google Cloud, (GCP: No estimate. My rough estimate is 3 days)</li>
  <li>Perform Foundational Data, ML, AI Tasks in Google Cloud (1 day)</li>
  <li>Build and Deploy Machine Learning Solutions on Vertex AI (1 day)</li>
  <li>Preparing for the Professional Machine Learning Engineer Examination (GCP: No estimate. My rough estimate is 5 days)</li>
</ul>
<p><strong>Total 40 days</strong></p>
<p>Below I share a link to a Google sheet tool I made to quickly estimate the time you've left in the program and the likely date for finishing the training, based on the time estimates of each course:</p>
<p><a href="https://docs.google.com/spreadsheets/d/1t7T9vL-H_ZUU0kbYCK3crnAUZeg5C7rjAGg_IEw3kpY/edit#gid=0">https://docs.google.com/spreadsheets/d/1t7T9vL-H_ZUU0kbYCK3crnAUZeg5C7rjAGg_IEw3kpY/edit#gid=0</a></p>
<h2>Issues with Labs</h2>
<p>This section lists issues encountered in the Qwiklab labs in the course and provides solutions or workarounds where possible.</p>
<p>&nbsp;</p>
<h3>Introduction to Vertex Pipelines</h3>
<p>Problem:<br>One of the pipeline containers fails with the error <code>TypeError: emojize() got an unexpected keyword argument 'use_aliases' </code>.</p>
<p>Solution:<br>Simply remove the <code>use_aliases</code> parameter from the <code>emojize()</code> function call in the emoji component.</p>
<p>&nbsp;</p>
<h3>Structured data prediction using Vertex AI Platform</h3>
<p>It isn't that much of a problem, since the prompts are there in the notebook, but this lab uses the old AI Platform rather than Vertex AI in places (despite the title), so you need to enable the AI Platform API (ml.googleapis.com) as well as the standard Vertex AI APIs, and then the model &amp; jobs are listed in the AI Platform web UI rather than the conventional Vertex AI location. E.g. https://console.cloud.google.com/ai-platform/jobs</p>
<p>&nbsp;</p>
<h3>Keras for Text Classification using Vertex AI</h3>
<p>Problem: <code>model.fit()</code> only runs for two epochs, resulting in poor accuracy and there being insufficient data to graph in the subsequent metric curve plots.<br>Solution: The early stopping control parameter <code>patience </code>is set to zero, so the training gives up immediately. Set the <code>PATIENCE </code>constant to a small number greater than zero.</p>
<p>&nbsp;</p>
<h3>NLP on Google Cloud, Lab 1: Exploring the Dialogflow API</h3>
<p>In Task 1, you should open DialogFlow via link: <a href="https://partner.cloudskillsboost.google/course_sessions/2413062/labs/dialogflow.cloud.google.com">dialogflow.cloud.google.com,</a> but you'll encounter a “Page not found” error. Following link should work: &nbsp;<a href="https://dialogflow.cloud.google.com/#/getStarted">https://dialogflow.cloud.google.com/#/getStarted</a></p>
<p>&nbsp;</p>
<h3>ML on GCP: Hybrid Recommendations with the MovieLens Dataset</h3>
<p>The instructions at the start of the lab regarding running a “previous” lab are a bit vague and misleading. Enter the correct project ID then run all of the code cells in als_bqml.ipynb from the solutions folder to import the data to the BQ dataset.</p>
<p>&nbsp;</p>
<h3>Applying Contextual Bandits for Recommendation systems using Tensorflow and Cloud Storage</h3>
<p>I encountered a number of package dependency and permissions issues.&nbsp;</p>
<p>My hacky workaround:</p>
<p>Add the <code>--user</code> option to the two <code>pip install</code>s, then ignore all of the pip warnings and errors.</p>
<p>Execute &nbsp;</p>
<p><code>!sudo rm -f /opt/conda/lib/python3.7/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so</code></p>
<p>before the code cell with the main imports.</p>
<p>There are also lots of variables set to None or 0 that need to be populated as indicated in the markup text, but without <code>TODO</code>s.&nbsp;</p>
<p>&nbsp;</p>
<h3>Build and Deploy Machine Learning Solutions with Vertex AI: Challenge Lab</h3>
<p>The instructions specify to keep the defaults when creating a Jupyter notebook and there was a pre-existing storage bucket with the correct name. However, for me, they were in different regions, which doesn't work. I don't know if you need to keep the default notebook region (probably not) but creating a bucket in the same region as the notebook worked for me.</p>
<p>&nbsp;</p>
<h3>TFX on Cloud AI Platform Pipelines</h3>
<p>TIP: Don't leave a trailing forward slash on the <code>ARTIFACT_STORE_URI</code> string referencing the GCS bucket, as it will cause the last stage of the Kubeflow pipeline to fail.</p>
<p>&nbsp;</p>
<h3>Continuous Training Pipelines with Cloud Composer</h3>
<p>When you attempt to run the command to create an Airflow environment, Composer comes back with the following error:<br><code>ERROR: (gcloud.composer.environments.create) Cannot specify --python-version with Composer 2.X or greater.</code></p>
<p>Simply removing the python version results in a different error and the lab appears to have been built around Composer v1 and Airflow v1, therefore run the following command instead:</p>
<pre><code class="language-plaintext">gcloud composer environments create demo-environment \
--location $REGION \
--python-version 3 \
--image-version=composer-1-airflow-1</code></pre>
<p>In the DAG code, the BigQuery query doesn't go back far enough since the data in the table (at the time of writing) ends in October 2022, so extend the lookback period, e.g.: <code>TIMESTAMP('{{ macros.ds_add(ds, -180) }}')</code></p>
<p>&nbsp;</p>
<h2>Experience of actual exam</h2>
<p>&nbsp;</p>
<h3>Brian Donaghy (April 2023)</h3>
<p>&nbsp;</p>
<h4>Summary</h4>
<p>Overall, there were lots of questions with numerous reasonable approaches, so you often find yourself picking the best candidate from two or three perfectly viable options, or picking the least bad option from two or three suboptimal sounding options. So I found doing practice exams where you have to make those sorts of choices, carefully parsing the language in the question helpful, noting that the actual questions in the third party practice exams may or may not be representative of the content of the real exam. I did not see any questions I'd seen before in real exam.</p>
<p>&nbsp;</p>
<h4>Resources I found useful</h4>
<p>Official Exam Guide:&nbsp;<br>https://cloud.google.com/certification/guides/machine-learning-engineer</p>
<p>Official Sample Questions:&nbsp;<br>https://docs.google.com/forms/d/e/1FAIpQLSeYmkCANE81qSBqLW0g2X7RoskBX9yGYQu-m1TtsjMvHabGqg/viewform</p>
<p>Good list of resources:<br>https://github.com/sathishvj/awesome-gcp-certifications/blob/master/professional-machine-learning-engineer.md</p>
<p>&nbsp;</p>
<p>GCP Docs:<br>https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform<br>https://cloud.google.com/tpu/docs</p>
<p>Best Practices:<br>https://cloud.google.com/architecture/ml-on-gcp-best-practices<br>https://cloud.google.com/architecture/best-practices-for-ml-performance-cost<br>https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build</p>
<p>&nbsp;</p>
<p>Practice Exam/$20 (just the exams, didn't get the other material) :<br>https://www.whizlabs.com/professional-machine-learning-engineer</p>
<p>Udemy course for exam prep/ &lt;$20 :<br>https://www.udemy.com/course/google-cloud-machine-learning-engineer-certification-prep</p>
<p>&nbsp;</p>
<h4>Specific types of questions I encountered</h4>
<p>Dealing with missing data, picking between removing rows with missing data and different imputation methods &nbsp;</p>
<p>Interfaces between different components</p>
<p>Lots about TFX &amp; Kubeflow</p>
<p>Selecting GPUs &amp; TPUs</p>
<p>Choosing between GPUs &amp; TPUs</p>
<p>When to use Precision or Recall</p>
<p>How to deal with imbalanced classes</p>
<p>Tensorflow Model Analysis tool</p>
<p>How to deal with some random error messages. E.g. an out of memory error &nbsp;</p>
<p>A few questions about PII and the Cloud Data Loss Prevention API</p>
<p>Quite a bit on Rec Sys but at a fairly high level. Nothing about WALS or the inner workings of matrix factorization etc.</p>
<p>Nothing on Keras. A lot of options surrounding TF at a very high level. A couple of deep in the weeds TF questions but generally nothing really at a coding level.</p>
<p>A few high level CV questions.</p>
<p>Nothing on RL.&nbsp;</p>
<p>Nothing really about tertiary GCP AI services like Contact Center AI etc.</p>
<p>&nbsp;</p>
<h4>Some specific questions that I encountered</h4>
<ol>
  <li>You want to reduce memory footprint of model in prod with minimal reduction in performance and without re-training model. I *think* I chose correctly to quantize the model from float32 to bfloat16 using TPUs.</li>
  <li>A hate speech classifier is producing false positives due to general religious discussion. How do you deal with it? I said add synthetic data to training.</li>
  <li>What does "The resource 'projects/deeplearning-platform/zones/europe-west4-c/acceleratorTypes/nvidia-tesla-k80' was not found" mean The two most plausible options were: there are no K80s in the region/zone specified or the GPU quota has been exceeded. I guessed correctly, picking the former, confirmed by the docs. Pulled from https://cloud.google.com/deep-learning-vm/docs/troubleshooting</li>
</ol>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
